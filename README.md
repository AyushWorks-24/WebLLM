**ğŸŒ WebLLM â€” Run LLMs Directly in Your Browser with WebGPU**

<p align="center"> <img src="https://img.shields.io/badge/WebGPU-Enabled-blue" /> <img src="https://img.shields.io/badge/AI-Local%20Inference-brightgreen" /> <img src="https://img.shields.io/badge/License-MIT-orange" /> </p>

WebLLM is a lightweight, privacy-focused project that lets you run modern Large Language Models entirely inside your browser â€” powered by WebGPU.
No backend. No API keys. No server cost.
Everything runs locally on your device. âš¡

ğŸš€ Key Highlights

âš¡ 100% Client-Side Inference

ğŸ”’ Zero Data Upload â€” Full Privacy

ğŸ§  Runs LLaMA, Gemma, Mistral, Phi, and more

ğŸ’» WebGPU acceleration â†’ Fast inference

ğŸ“¦ Simple API for chat & text generation

ğŸŒ Works on Chrome, Edge, Brave (WebGPU-enabled)

ğŸ“¸ Demo Preview

(Add a screenshot or GIF here for a polished look)
Example:

/public/demo.png

ğŸ“ Project Structure
WebLLM/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ components/    # UI components
â”‚   â”œâ”€â”€ models/        # Model loaders & config
â”‚   â””â”€â”€ utils/         # Helper functions
â”‚â”€â”€ public/            # Static assets + model files
â”‚â”€â”€ package.json
â”‚â”€â”€ vite.config.js
â””â”€â”€ README.md

âš™ï¸ Installation
Clone the repository
git clone https://github.com/AyushWorks-24/WebLLM
cd WebLLM

Install dependencies
npm install

Start development server
npm run dev


Now open the link shown in the terminal (usually: http://localhost:5173/).

ğŸ¤– Using WebLLM in Your Project

Example of loading a model and generating text:

import { CreateWebLLMChat } from "@mlc-ai/web-llm";

async function run() {
  const chat = await CreateWebLLMChat("Llama-3-8B");
  const output = await chat.generate("Hello WebLLM!");
  console.log(output);
}

run();

ğŸŒŸ Supported Models

WebLLM supports a wide range of models:

LLaMA 3

Mistral

Gemma

Phi-2 / Phi-3

Qwen

And more depending on browser memory limits.

ğŸ› ï¸ Browser Requirements

To run LLMs locally:

âœ” Chrome 113+ / Edge 113+

âœ” WebGPU enabled

âœ” Good GPU recommended (but not mandatory)

Check if WebGPU works:

chrome://flags â†’ search â€œWebGPUâ€ â†’ Enable

ğŸ“Œ Roadmap

 UI redesign

 Multi-model selector

 Offline model caching

 Voice input (Speech-to-Text)

 Chat history + export

 Dark / Light theme switch

ğŸ¤ Contributing

Contributions are welcome!
If you have ideas or improvements, feel free to:

Open an Issue

Create a Pull Request

ğŸ“„ License

This project is licensed under the MIT License.

ğŸ‘¤ Author

AyushWorks-24
GitHub: https://github.com/AyushWorks-24
